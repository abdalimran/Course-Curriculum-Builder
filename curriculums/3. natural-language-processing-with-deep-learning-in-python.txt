1. Outline, Review, and Logistical Things
	1.1 Introduction, Outline, and Review
	1.2 How to Succeed in this Course
	1.3 Where to get the code / data for this course
	1.4 Preprocessed Wikipedia Data
	1.5 How to Open Files for Windows Users
2. Beginner's Corner: Working with Word Vectors
	2.1 What are vectors?
	2.2 What is a word analogy?
	2.3 Trying to find and assess word vectors using TF-IDF and t-SNE
	2.4 Pretrained word vectors from GloVe
	2.5 Pretrained word vectors from word2vec
	2.6 Text Classification with word vectors
	2.7 Text Classification in Code
	2.8 Using pretrained vectors later in the course
	2.9 Suggestion Box
3. Review of Language Modeling and Neural Networks
	3.1 Review Section Intro
	3.2 Bigrams and Language Models
	3.3 Bigrams in Code
	3.4 Neural Bigram Model
	3.5 Neural Bigram Model in Code
	3.6 Neural Network Bigram Model
	3.7 Neural Network Bigram Model in Code
	3.8 Improving Efficiency
	3.9 Improving Efficiency in Code
	3.10 Review Section Summary
4. Word Embeddings and Word2Vec
	4.1 Return of the Bigram
	4.2 CBOW
	4.3 Skip-Gram
	4.4 Hierarchical Softmax
	4.5 Negative Sampling
	4.6 Negative Sampling - Important Details
	4.7 Why do I have 2 word embedding matrices and what do I do with them?
	4.8 Word2Vec implementation tricks
	4.9 Word2Vec implementation outline
	4.10 Word2Vec in Code with Numpy
	4.11 Tensorflow or Theano - Your Choice!
	4.12 Word2Vec Tensorflow Implementation Details
	4.13 Word2Vec Tensorflow in Code
	4.14 Alternative to Wikipedia Data: Brown Corpus
5. Word Embeddings using GloVe
	5.1 GloVe Section Introduction
	5.2 Matrix Factorization for Recommender Systems - Basic Concepts
	5.3 Matrix Factorization Training
	5.4 Expanding the Matrix Factorization Model
	5.5 Regularization for Matrix Factorization
	5.6 GloVe - Global Vectors for Word Representation
	5.7 Recap of ways to train GloVe
	5.8 GloVe in Code - Numpy Gradient Descent
	5.9 GloVe in Code - Alternating Least Squares
	5.10 GloVe in Tensorflow with Gradient Descent
	5.11 Visualizing country analogies with t-SNE
	5.12 Hyperparameter Challenge
	5.13 Training GloVe with SVD (Singular Value Decomposition)
6. Unifying Word2Vec and GloVe
	6.1 Pointwise Mutual Information - Word2Vec as Matrix Factorization
	6.2 PMI in Code
7. Using Neural Networks to Solve NLP Problems
	7.1 Parts-of-Speech (POS) Tagging
	7.2 How can neural networks be used to solve POS tagging?
	7.3 Parts-of-Speech Tagging Baseline
	7.4 Parts-of-Speech Tagging Recurrent Neural Network in Theano
	7.5 Parts-of-Speech Tagging Recurrent Neural Network in Tensorflow
	7.6 How does an HMM solve POS tagging?
	7.7 Parts-of-Speech Tagging Hidden Markov Model (HMM)
	7.8 Named Entity Recognition (NER)
	7.9 Comparing NER and POS tagging
	7.10 Named Entity Recognition Baseline
	7.11 Named Entity Recognition RNN in Theano
	7.12 Named Entity Recognition RNN in Tensorflow
	7.13 Hyperparameter Challenge II
8. Recursive Neural Networks (Tree Neural Networks)
	8.1 Recursive Neural Networks Section Introduction
	8.2 Sentences as Trees
	8.3 Data Description for Recursive Neural Networks
	8.4 What are Recursive Neural Networks / Tree Neural Networks (TNNs)?
	8.5 Building a TNN with Recursion
	8.6 Trees to Sequences
	8.7 Recursive Neural Tensor Networks
	8.8 RNTN in Tensorflow (Tips)
	8.9 RNTN in Tensorflow (Code)
	8.10 Recursive Neural Network in TensorFlow with Recursion
9. Theano and Tensorflow Basics Review
	9.1 (Review) Theano Basics
	9.2 (Review) Theano Neural Network in Code
	9.3 (Review) Tensorflow Basics
	9.4 (Review) Tensorflow Neural Network in Code
10. Setting Up Your Environment (FAQ by Student Request)
	10.1 Anaconda Environment Setup
	10.2 How to install Numpy, Scipy, Matplotlib, Pandas, IPython, Theano, and TensorFlow
